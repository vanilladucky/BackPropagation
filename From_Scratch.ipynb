{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np \n",
    "import torchinfo \n",
    "from torchinfo import summary \n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equation(input:torch.Tensor): # z = 5 * a^2 + 3 * b^2\n",
    "    return torch.matmul(torch.mul(input, input), torch.tensor([5, 3], dtype=torch.float32))\n",
    "\n",
    "num_of_data = 100000\n",
    "num_of_train_data = int(num_of_data * 0.7)\n",
    "num_of_val_data =int(num_of_data * 0.3)\n",
    "inputs = torch.randn(num_of_data, 2)\n",
    "outputs = equation(inputs).view(-1,1)\n",
    "\n",
    "batch_size = 500 \n",
    "number_of_batches = num_of_data//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLearning():\n",
    "\n",
    "    def __init__(self, n_inputs, n_hidden, n_outputs, lr = 0.01):\n",
    "        self.lr = lr\n",
    "        # Initilizaing empty layers         \n",
    "        self.hidden_layer = torch.randn(n_hidden, n_inputs+1) # additional 1 is to account for the bias value\n",
    "        self.output_layer = torch.randn(n_outputs, n_hidden+1) # additional 1 is to account for the bias value\n",
    "\n",
    "    def activate(self, weights, inputs):\n",
    "        return torch.matmul(inputs, torch.transpose(weights[:, :-1], 0, 1)) + weights[:, -1]\n",
    "\n",
    "    def sigmoid(self, inputs):\n",
    "        return 1.0 / (1.0 + np.exp(-inputs))\n",
    "\n",
    "    def MSELoss(self, target, output):\n",
    "        return torch.mean((target-output)**2)\n",
    "\n",
    "    def forwardpropagate(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.target = targets\n",
    "        out = self.activate(self.hidden_layer, self.inputs)\n",
    "        out = self.sigmoid(out)\n",
    "        self.hidden_nodes = out.clone()\n",
    "        out = self.activate(self.output_layer, out)\n",
    "        self.forward_res = out\n",
    "        return self.MSELoss(self.target, out).item()\n",
    "\n",
    "    def backpropagate(self): # BackPropagation       \n",
    "\n",
    "        # Accounting for the output_layer, from 10 nodes to one single output node\n",
    "        dc_dw2 = torch.mul(2 * (self.forward_res-self.target), self.hidden_nodes)\n",
    "        dc_db2 = 2 * (self.forward_res-self.target)\n",
    "        dc_dW2 = torch.cat((dc_dw2, dc_db2), dim = 1)\n",
    "        dc_dW2 = torch.mean(dc_dW2, dim = 0).view(1,-1)\n",
    "\n",
    "        # Accouting for the hidden_layer, from the 2 input nodes to the 10 hidden nodes\n",
    "        sigmoid_derivative = torch.matmul(torch.transpose(self.hidden_nodes, 0, 1), (1-self.hidden_nodes))\n",
    "        dc_dy = 2*(self.forward_res-self.target)\n",
    "        dy_dhidden= self.output_layer[:, :-1]\n",
    "        dx_dw1 = self.inputs\n",
    "        dc_dW1 = torch.matmul(torch.transpose(torch.matmul(torch.matmul(dc_dy, dy_dhidden), sigmoid_derivative), 0, 1), dx_dw1)\n",
    "        dc_dW1 = torch.cat((dc_dW1, torch.mean((torch.transpose(torch.matmul(torch.matmul(dc_dy, dy_dhidden), sigmoid_derivative), 0, 1)), dim =1).view(-1,1)), dim=1)\n",
    "\n",
    "        # Optimizing\n",
    "        self.output_layer -= self.lr * dc_dW2 \n",
    "        self.hidden_layer -= self.lr * dc_dW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Epoch 0 | Training Loss : 89.00325297623229 | Validation Loss : 82.54726649138887 ====\n",
      "====Epoch 1 | Training Loss : 76.79070416457361 | Validation Loss : 75.14696987604691 ====\n",
      "====Epoch 2 | Training Loss : 68.7238460238889 | Validation Loss : 68.23674366837841 ====\n",
      "====Epoch 3 | Training Loss : 68.5056367778092 | Validation Loss : 68.21958729372186 ====\n",
      "====Epoch 4 | Training Loss : 68.50751731378568 | Validation Loss : 68.21832042629435 ====\n",
      "====Epoch 5 | Training Loss : 68.52732440207502 | Validation Loss : 68.22835211026467 ====\n",
      "====Epoch 6 | Training Loss : 68.57223798902773 | Validation Loss : 68.21666167954267 ====\n",
      "====Epoch 7 | Training Loss : 68.60415948552193 | Validation Loss : 68.23872582387116 ====\n",
      "====Epoch 8 | Training Loss : 68.7059502087051 | Validation Loss : 68.28537814900027 ====\n",
      "====Epoch 9 | Training Loss : 68.69425259048133 | Validation Loss : 68.54287719726562 ====\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val = inputs[:num_of_train_data], outputs[:num_of_train_data], inputs[num_of_train_data:], outputs[num_of_train_data:]\n",
    "model = DeepLearning(2, 10, 1, lr = 0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    # Training Loop - with backpropagation to adjust weights and biases \n",
    "    training_loss, count = 0, 0\n",
    "    for batch_end_idx in range(batch_size, num_of_train_data, batch_size):\n",
    "        train_input, train_output = X_train[batch_end_idx-batch_size:batch_end_idx], y_train[batch_end_idx-batch_size:batch_end_idx]\n",
    "        training_loss += model.forwardpropagate(train_input, train_output)\n",
    "        model.backpropagate()\n",
    "        count += 1\n",
    "    training_loss /= count \n",
    "\n",
    "    # Validation Loop - without backpropagation \n",
    "    validation_loss, count = 0, 0\n",
    "    for batch_end_idx in range(batch_size, num_of_val_data, batch_size):\n",
    "        val_input, val_output = X_val[batch_end_idx-batch_size:batch_end_idx], y_val[batch_end_idx-batch_size:batch_end_idx]\n",
    "        validation_loss += model.forwardpropagate(val_input, val_output)\n",
    "        count += 1\n",
    "    validation_loss /= count \n",
    "\n",
    "    print(f\"====Epoch {epoch} | Training Loss : {training_loss} | Validation Loss : {validation_loss} ====\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('machinelearning': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68339890f90b7cc04351217391b998bebccc50e4c246697aa46d0c3bd6df040a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
